{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datapipe.datatable import DataStore, DBConn\n",
    "from datapipe.compute import Catalog, Pipeline, Table, run_pipeline\n",
    "from datapipe.store.database import TableStoreDB\n",
    "from datapipe.store.redis import RedisStore\n",
    "from datapipe.core_steps import BatchTransform, UpdateExternalTable\n",
    "from datapipe.types import data_to_index\n",
    "from sqlalchemy.engine import create_engine\n",
    "from sqlalchemy import inspect\n",
    "from sqlalchemy import Column, String, JSON, Integer, Boolean\n",
    "import pandas as pd\n",
    "import redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4) (1, 4)\n",
      "(2, 5) (2, 5)\n",
      "(3, 6) (3, 6)\n"
     ]
    }
   ],
   "source": [
    "prim_key_a = ['a', 'b']\n",
    "prim_key_b = ['e', 'f']\n",
    "\n",
    "a = pd.DataFrame({'a': [1,2,3], 'b': [4,5,6], 'c': [7,8,9], 'd': ['a', 'b', 'c']})\n",
    "b = pd.DataFrame({'e': [1,2,3], 'f': [4,5,6], 'g': [7,8,9], 'h': ['a', 'b', 'c']})\n",
    "\n",
    "c = a[prim_key_a].itertuples(index=False, name=None)\n",
    "d = b[prim_key_b].itertuples(index=False, name=None)\n",
    "\n",
    "for keys, values in zip(c, d):\n",
    "    print(keys, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entities:\n",
    "# DBConn - database connection (conn url string or conn object itself. question - does dbconn only eat sqlalchemy?)\n",
    "# DataStore - metatables data storage (entity where hashes and info about changes is stored)\n",
    "# Table - data table (could be in db, or in file? question - are all sources of data described as Table?)\n",
    "# Catalog - entity that describes DataTables\n",
    "# Store - entity with methods for different kinds of storage.  could be TableStoreDB, TableDataSingleFileStore, folder etc...\n",
    "# TableStoreDB - table data stored in database.\n",
    "# Table schemas described in sqlaclchemy terms and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    }
   ],
   "source": [
    "DBCONN = \"postgresql://postgres:testpass@localhost:5432/test\"\n",
    "engine = create_engine(DBCONN)\n",
    "# two separate connections needed for datastore and main tables\n",
    "engine.execute('''\n",
    "DROP SCHEMA public CASCADE;\n",
    "CREATE SCHEMA public;''')\n",
    "dbconn = DBConn(DBCONN)\n",
    "meta_dbconn = DBConn(DBCONN)\n",
    "ds = DataStore(meta_dbconn)\n",
    "redis_conn_mid = redis.Redis(decode_responses=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MID PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SCHEMA = [\n",
    "    Column(\"user_id\", String, primary_key=True),\n",
    "    Column(\"guest_id\", String, primary_key=True),\n",
    "    Column(\"click_count\", Integer)\n",
    "]\n",
    "\n",
    "INTERM_SCHEMA = [\n",
    "    Column('user_id', String, primary_key=True),\n",
    "    Column('click_count_doubled', Integer)\n",
    "]\n",
    "\n",
    "OUTPUT_SCHEMA = [\n",
    "    Column(\"user_id\", String, primary_key=True),\n",
    "    Column(\"click_count_doubled\", Integer),\n",
    "    Column(\"click_count_doubled_squared\", Integer)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = Catalog({\n",
    "    \"pipeline_start\": Table(store=TableStoreDB(dbconn, \"test_input\", INPUT_SCHEMA)),\n",
    "    \"pipeline_mid\": Table(store=RedisStore(redis_conn_mid,'test_mid', INTERM_SCHEMA)),\n",
    "    \"pipeline_end\": Table(store=TableStoreDB(dbconn, \"test_output\", OUTPUT_SCHEMA))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double(df):\n",
    "    df['click_count_doubled'] = df['click_count'] * 2\n",
    "    df.drop(columns=['click_count', 'guest_id'], inplace=True)\n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "def double_and_square(df):\n",
    "    print(df)\n",
    "    df['click_count_doubled_squared'] = df['click_count_doubled']**2\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    UpdateExternalTable('pipeline_start'),\n",
    "    BatchTransform(\n",
    "        double,\n",
    "        inputs=[\"pipeline_start\"],\n",
    "        outputs=[\"pipeline_mid\"],\n",
    "    ),\n",
    "    BatchTransform(\n",
    "        double_and_square,\n",
    "        inputs=['pipeline_mid'],\n",
    "        outputs=['pipeline_end']\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 27.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [user_id, guest_id, click_count]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [user_id, click_count_doubled, click_count_doubled_squared]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_pipeline(ds, catalog, pipeline)\n",
    "input_dt = catalog.get_datatable(ds, \"pipeline_start\")\n",
    "output_dt = catalog.get_datatable(ds, \"pipeline_end\")\n",
    "print(input_dt.get_data())\n",
    "print(output_dt.get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 11.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  user_id  click_count_doubled\n",
      "0     abc                  246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  user_id  click_count_doubled\n",
      "0     abc                  246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "engine.execute('''\n",
    "insert into test_input values ('abc', 'guest_id', 123)\n",
    "''')\n",
    "run_pipeline(ds, catalog, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  user_id  guest_id  click_count\n",
      "0     abc  guest_id          123\n",
      "  user_id  click_count_doubled\n",
      "0     abc                  246\n",
      "  user_id  click_count_doubled  click_count_doubled_squared\n",
      "0     abc                  246                        60516\n"
     ]
    }
   ],
   "source": [
    "input_dt = catalog.get_datatable(ds, \"pipeline_start\")\n",
    "mid_dt = catalog.get_datatable(ds, 'pipeline_mid')\n",
    "output_dt = catalog.get_datatable(ds, \"pipeline_end\")\n",
    "print(input_dt.get_data())\n",
    "print(mid_dt.get_data())\n",
    "print(output_dt.get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 18.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [user_id, guest_id, click_count]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [user_id, click_count_doubled]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [user_id, click_count_doubled, click_count_doubled_squared]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "engine.execute('''\n",
    "delete from test_input where user_id = 'abc'\n",
    "''')\n",
    "run_pipeline(ds, catalog, pipeline)\n",
    "input_dt = catalog.get_datatable(ds, \"pipeline_start\")\n",
    "mid_dt = catalog.get_datatable(ds, 'pipeline_mid')\n",
    "output_dt = catalog.get_datatable(ds, \"pipeline_end\")\n",
    "print(input_dt.get_data())\n",
    "print(mid_dt.get_data())\n",
    "print(output_dt.get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d39f20ce00aaf38ea377c8d0bf80d1ffd1fae8bb8a33e458e99c5fd845ceed94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
